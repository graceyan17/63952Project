{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "R1AtYu8ohc8O"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation functions for recommender PERFORMANCE"
      ],
      "metadata": {
        "id": "TRqedWw0h70S"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_test_set(test_df, min_user_test_entries):\n",
        "  \"\"\"\n",
        "  filters test set to only include users for whom we have enough data to evaluate them on\n",
        "  \"\"\"\n",
        "  counts = test_df['user_id'].value_counts()\n",
        "  from collections import Counter\n",
        "  counter = Counter(counts)\n",
        "  print(counter)\n",
        "  keep_users = counts[counts >= min_user_test_entries].index\n",
        "  return test_df[test_df['user_id'].isin(keep_users)]"
      ],
      "metadata": {
        "id": "IRmfPpkskJlf"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def highly_rated_recall_at_k(user_recommendations, train_df, test_df, user_id, k=20):\n",
        "  \"\"\"\n",
        "  user_recommendations: list of book recommendations for the user\n",
        "  train_df: dataset to make recommendations\n",
        "  test_df: validation dataset\n",
        "  user_id: the user's id\n",
        "  k: number of books to recommend\n",
        "\n",
        "  computes recall @ k for highly rated books\n",
        "  \"\"\"\n",
        "  # get set of \"ground truth books\" (books that appear for the user in the test set, which they have rated >= 7)\n",
        "  user_ratings_test = test_df[test_df.user_id == user_id]\n",
        "  if len(user_ratings_test) == 0:\n",
        "    return 0\n",
        "  highly_rated = user_ratings_test[user_ratings_test['rating'] >= 7]\n",
        "  ground_truth = set(highly_rated['join_title'])\n",
        "\n",
        "  num_matches = len([book for book in user_recommendations if book in ground_truth])\n",
        "  user_test_books = len(ground_truth)\n",
        "\n",
        "  return num_matches / user_test_books if user_test_books else 0\n"
      ],
      "metadata": {
        "id": "ueT6nAHgh-TW"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(user_recommendations, train_df, test_df, user_id, k=20):\n",
        "  \"\"\"\n",
        "  user_recommendations: list of book recommendations for the user\n",
        "  train_df: dataset to make recommendations\n",
        "  test_df: validation dataset\n",
        "  user_id: the user's id\n",
        "  k: number of books to recommend\n",
        "\n",
        "  computes recall @ k\n",
        "  \"\"\"\n",
        "  # get set of \"ground truth books\" (books that appear for the user in the test set)\n",
        "  user_ratings_test = test_df[test_df.user_id == user_id].copy()\n",
        "  if len(user_ratings_test) == 0:\n",
        "    return 0\n",
        "\n",
        "  ground_truth = set(user_ratings_test['join_title'])\n",
        "\n",
        "  num_matches = len([book for book in user_recommendations if book in ground_truth])\n",
        "  user_test_books = len(ground_truth)\n",
        "\n",
        "  return num_matches / user_test_books if user_test_books else 0"
      ],
      "metadata": {
        "id": "N4MVS28-h--C"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(user_recommendations, train_df, test_df, user_id, k=20):\n",
        "  \"\"\"\n",
        "  user_recommendations: list of book recommendations for the user\n",
        "  train_df: dataset to make recommendations\n",
        "  test_df: validation dataset\n",
        "  user_id: the user's id\n",
        "  k: number of books to recommend\n",
        "\n",
        "  computes precision @ k\n",
        "  \"\"\"\n",
        "  # get set of \"ground truth books\" (books that appear for the user in the test set)\n",
        "  user_test = test_df[test_df.user_id == user_id]\n",
        "  if len(user_test) == 0:\n",
        "      return 0\n",
        "  ground_truth = set(user_test['join_title'])\n",
        "\n",
        "  num_matches = len([book for book in user_recommendations if book in ground_truth])\n",
        "\n",
        "  return num_matches / k"
      ],
      "metadata": {
        "id": "lH8SafoCiBGv"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def highly_rated_precision_at_k(user_recommendations, train_df, test_df, user_id, k=20):\n",
        "  \"\"\"\n",
        "  user_recommendations: list of book recommendations for the user\n",
        "  train_df: dataset to make recommendations\n",
        "  test_df: validation dataset\n",
        "  user_id: the user's id\n",
        "  k: number of books to recommend\n",
        "\n",
        "  computes precision @ k for highly rated books\n",
        "  \"\"\"\n",
        "  # get set of \"ground truth books\" (books that appear for the user in the test set, which they have rated >= 7)\n",
        "  user_test = test_df[test_df.user_id == user_id]\n",
        "  if len(user_test) == 0:\n",
        "      return 0\n",
        "  ground_truth = set(user_test[user_test.rating >= 7]['join_title'])\n",
        "\n",
        "  num_matches = len([book for book in user_recommendations if book in ground_truth])\n",
        "\n",
        "  return num_matches / k"
      ],
      "metadata": {
        "id": "4vTvfGUxlOi2"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score(precision, recall):\n",
        "  \"\"\"\n",
        "  computes f1 score from precision and recall\n",
        "  \"\"\"\n",
        "  return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0"
      ],
      "metadata": {
        "id": "t8jL8V9jn9g5"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(recommendations, train_df, test_df, k=20):\n",
        "  \"\"\"\n",
        "  recommendations: a dictionary mapping user_ids (strings) to lists of book recommendations for each user\n",
        "  train_df: dataset to make recommendations\n",
        "  test_df: validation dataset\n",
        "  user_id: the user's id\n",
        "  k: number of books to recommend\n",
        "  \"\"\"\n",
        "  # get data for only evaluation users\n",
        "  valid_user_ids = list(set(train_df.user_id.unique()) & set(test_df.user_id.unique()))\n",
        "\n",
        "  eval_user_ids = list(\n",
        "    set(map(int, recommendations.keys())) &\n",
        "    set(test_df.user_id.unique())\n",
        "  )\n",
        "\n",
        "  highly_rated_recalls = []\n",
        "  recalls = []\n",
        "  precisions = []\n",
        "  highly_rated_precisions = []\n",
        "  for user_id in tqdm(eval_user_ids):\n",
        "    user_id_str = str(user_id)\n",
        "    user_recommendations = recommendations[user_id_str]\n",
        "    highly_rated_recall = highly_rated_recall_at_k(user_recommendations, train_df, test_df, user_id, k=20)\n",
        "    recall = recall_at_k(user_recommendations, train_df, test_df, user_id, k=20)\n",
        "    precision = precision_at_k(user_recommendations, train_df, test_df, user_id, k=20)\n",
        "    highly_rated_precision = highly_rated_precision_at_k(user_recommendations, train_df, test_df, user_id, k=20)\n",
        "    highly_rated_recalls.append(highly_rated_recall)\n",
        "    recalls.append(recall)\n",
        "    precisions.append(precision)\n",
        "    highly_rated_precisions.append(highly_rated_precision)\n",
        "\n",
        "  f1_scores = [f1_score(p, r) for p, r in zip(precisions, recalls)]\n",
        "  avg_f1_score = np.mean(f1_scores)\n",
        "  highly_rated_f1_scores = [f1_score(p, r) for p, r in zip(highly_rated_precisions, highly_rated_recalls)]\n",
        "  avg_highly_rated_f1_score = np.mean(highly_rated_f1_scores)\n",
        "\n",
        "  avg_highly_rated_recall = np.mean(highly_rated_recalls)\n",
        "  avg_recall = np.mean(recalls)\n",
        "  avg_precision = np.mean(precisions)\n",
        "  avg_highly_rated_precision = np.mean(highly_rated_precisions)\n",
        "\n",
        "  print(f\"\\nAverage recall@k={k}: {avg_recall} for k = {k}\")\n",
        "  print(f\"\\nAverage highly rated recall@k={k}: {avg_highly_rated_recall}\")\n",
        "  print(f\"\\nAverage precision@k={k}: {avg_precision}\")\n",
        "  print(f\"\\nAverage highly rated precision@k={k}: {avg_highly_rated_precision}\")\n",
        "  print(f\"\\nAverage f1@k={k}: {avg_f1_score}\")\n",
        "  print(f\"\\nAverage highly rated f1@k={k}: {avg_highly_rated_f1_score}\")\n",
        "  print(f\"\\nNumber of users evaluated: {len(eval_user_ids)}\")\n"
      ],
      "metadata": {
        "id": "oVyQfKYAiDpf"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load datasets\n",
        "train_df = pd.read_csv('train_edges.csv')\n",
        "test_df = pd.read_csv('test_edges.csv')"
      ],
      "metadata": {
        "id": "Eb9f5_EKicGU"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.rename(columns={'User-ID':'user_id','Book-Rating': 'rating'})\n",
        "test_df = test_df.rename(columns={'User-ID':'user_id','Book-Rating': 'rating'})"
      ],
      "metadata": {
        "id": "GA0iDrzLiev8"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before filtering test data:\")\n",
        "valid_user_ids = list(set(train_df.user_id.unique()) & set(test_df.user_id.unique()))\n",
        "print(f\"Evaluation users: {len(valid_user_ids)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM5ugm00vvgW",
        "outputId": "2dad58ca-d03f-4333-95a4-e2f14fd14589"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before filtering test data:\n",
            "Evaluation users: 9055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_user_test_entries = 5\n",
        "filtered_test_df = filter_test_set(test_df, min_user_test_entries)"
      ],
      "metadata": {
        "id": "xAQDroRVnj-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cfdbb2f-6844-40e1-9194-660fbb548f2c"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({1: 4986, 2: 1458, 3: 681, 4: 416, 5: 269, 6: 199, 7: 130, 8: 126, 9: 96, 10: 72, 11: 64, 13: 59, 12: 54, 14: 46, 15: 35, 16: 34, 17: 32, 20: 25, 18: 23, 21: 20, 22: 19, 19: 17, 30: 15, 23: 14, 26: 13, 25: 13, 35: 10, 37: 8, 32: 8, 27: 8, 34: 7, 33: 7, 24: 7, 31: 6, 48: 5, 46: 5, 28: 5, 51: 4, 42: 4, 40: 4, 29: 4, 76: 3, 43: 3, 84: 2, 80: 2, 62: 2, 53: 2, 50: 2, 49: 2, 45: 2, 44: 2, 39: 2, 38: 2, 36: 2, 326: 1, 117: 1, 104: 1, 93: 1, 92: 1, 86: 1, 82: 1, 79: 1, 71: 1, 70: 1, 66: 1, 59: 1, 58: 1, 57: 1, 56: 1, 54: 1, 52: 1, 47: 1, 41: 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After filtering test data:\")\n",
        "valid_user_ids = list(set(train_df.user_id.unique()) & set(filtered_test_df.user_id.unique()))\n",
        "print(f\"Evaluation users: {len(valid_user_ids)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rcgf1AVCv8KE",
        "outputId": "4dc7573f-b5e1-4c09-f221-ac593977ae9c"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After filtering test data:\n",
            "Evaluation users: 1514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxIc3VOwgY2j",
        "outputId": "ecc9c285-5511-4d10-ac63-2ff2b5184d5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data type: <class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "# read in recommendations from json file\n",
        "# file_path = \"collaborative_filtering_recommendations_k_50_users_100.json\"\n",
        "# file_path = \"item_based_collaborative_filtering_recommendations_k_50_items_50.json\"\n",
        "file_path = \"page_rank.json\"\n",
        "with open(file_path) as json_file:\n",
        "  recommendations = json.load(json_file)\n",
        "  print(\"data type:\", type(recommendations))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run evaluation: recall, precision, f1\n",
        "k = 50\n",
        "# evaluate on original test data\n",
        "evaluate(recommendations, train_df, test_df, k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBHjHCUQiF8V",
        "outputId": "bfa1eb44-6b5f-4825-8ed3-55d6b0e10906"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9055/9055 [00:19<00:00, 475.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average recall@k=50: 0.29065177706116685 for k = 50\n",
            "\n",
            "Average highly rated recall@k=50: 0.1771406020308338\n",
            "\n",
            "Average precision@k=50: 0.04656543346217559\n",
            "\n",
            "Average highly rated precision@k=50: 0.016963003865267806\n",
            "\n",
            "Average f1@k=50: 0.06460561581984817\n",
            "\n",
            "Average highly rated f1@k=50: 0.028619390069422174\n",
            "\n",
            "Number of users evaluated: 9055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on filtered test data\n",
        "evaluate(recommendations, train_df, filtered_test_df, k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKz2uK2F-abj",
        "outputId": "e0c3a5b9-2bd0-4a9f-e6cb-38f8cb4109c8"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1514/1514 [00:02<00:00, 506.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average recall@k=50: 0.27142349710845387 for k = 50\n",
            "\n",
            "Average highly rated recall@k=50: 0.26525197141514734\n",
            "\n",
            "Average precision@k=50: 0.16720607661822987\n",
            "\n",
            "Average highly rated precision@k=50: 0.05227873183619551\n",
            "\n",
            "Average f1@k=50: 0.18359030026602535\n",
            "\n",
            "Average highly rated f1@k=50: 0.07949745409227735\n",
            "\n",
            "Number of users evaluated: 1514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation functions for recommendation BIAS? tbd"
      ],
      "metadata": {
        "id": "znE1Em_kfi-g"
      },
      "execution_count": 262,
      "outputs": []
    }
  ]
}